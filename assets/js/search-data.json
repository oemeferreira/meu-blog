{
  
    
        "post0": {
            "title": "Steve Jobs e Steve Wozniak",
            "content": "Escolha da Foto . Para a escolha da foto pensei que teria que ser alguma coisa de que eu gostasse e alguma coisa que fosse importante na história. . Então como gosto bastante de tecnologia, pesnsei em alguma foto de alguma pessoa importante para a indústria, e depois de muito procurar, lembrei de Steve Jobs. E acabei encontrando essa foto com Steve Wozniak. . Steven Jobs foi um inventor, empresário e magnata americano no setor da informática. Notabilizou-se como co-fundador, presidente e diretor executivo da Apple Inc. e por revolucionar seis indústrias: computadores pessoais, filmes de animação, música, telefones, tablets e publicações digitais. . Steve Wozniak é um engenheiro eletrônico e programador de computadores, co-fundador da Apple, Inc. . Jobs e Wozniak, entre outros, desenvolveram e comercializam uma das primeiras linhas de computadores pessoais de sucesso, a série Apple II. . Portanto com esses caras incríveis, resolvi realizar esse exercício de colorização utilizando o python na ferramenta colaboratory do Google. . Importando Deoldify Wrapper do Github . !wget https://raw.githubusercontent.com/awarischool/br-data-science/master/image-colorizer/deoldify_wrapper.py . --2022-05-06 01:30:49-- https://raw.githubusercontent.com/awarischool/br-data-science/master/image-colorizer/deoldify_wrapper.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2623 (2.6K) [text/plain] Saving to: ‘deoldify_wrapper.py’ deoldify_wrapper.py 100%[===================&gt;] 2.56K --.-KB/s in 0s 2022-05-06 01:30:49 (31.4 MB/s) - ‘deoldify_wrapper.py’ saved [2623/2623] . Importando biblioteca DeOldify . from deoldify_wrapper import DeOldify . Importing Libraries No module named &#39;deoldify&#39; DeOldify not found, installing.. Cloning DeOldify Repository... Opening DeOldify Folder Importing Libraries . /content/deoldify_wrapper.py:36: UserWarning: WARNING: GPU not available. Activate it on Colab at Edit &gt; Notebook Settings warnings.warn(&#39;WARNING: GPU not available. Activate it on Colab at Edit &gt; Notebook Settings&#39;) . Installing Colab requirements... Importing DeOldify Visualize module and FastAI Downloading Colorizer Model . Definindo a fun&#231;&#227;o . deo = DeOldify() . Initializing Colorizer . /usr/local/lib/python3.7/dist-packages/fastai/data_block.py:442: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning. warn(&#34;Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.&#34;) /usr/local/lib/python3.7/dist-packages/fastai/data_block.py:445: UserWarning: Your validation set is empty. If this is by design, use `split_none()` or pass `ignore_empty=True` when labelling to remove this warning. or pass `ignore_empty=True` when labelling to remove this warning.&#34;&#34;&#34;) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Downloading: &#34;https://download.pytorch.org/models/resnet101-63fe2227.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth . Done! . Colorizando a foto . deo.colorize(&#39;https://catracalivre.com.br/wp-content/uploads/2012/03/steve_jobs-steve-wozniak-bbc-reproducao.jpg&#39;) .",
            "url": "https://oemeferreira.github.io/meu-blog/awari/datacience/jobs/wozniak/2022/06/10/_05_05_Steve_Jobs_e_Steve_Wozniak.html",
            "relUrl": "/awari/datacience/jobs/wozniak/2022/06/10/_05_05_Steve_Jobs_e_Steve_Wozniak.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Projeto Final de Data Science - Awari",
            "content": "Importa&#231;&#227;o de Pacotes . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import plotly.express as px from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, precision_score, recall_score, classification_report, precision_recall_curve, average_precision_score, roc_curve, auc from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.linear_model import LogisticRegression import warnings warnings.filterwarnings(&#39;ignore&#39;) . Leitura dos dados . from google.colab import drive drive.mount(&#39;/content/drive&#39;) df = pd.read_csv(&#39;/content/drive/My Drive/AWARI/Projeto/dados_bc_2021_11.csv&#39;, sep=&#39;;&#39;) df.head() . Mounted at /content/drive . uf tcb sr cliente cnae_secao cnae_subclasse porte modalidade origem indexador ... a_vencer_ate_90_dias a_vencer_de_91_ate_360_dias a_vencer_de_361_ate_1080_dias a_vencer_de_1081_ate_1800_dias a_vencer_de_1801_ate_5400_dias a_vencer_acima_de_5400_dias vencido_acima_de_15_dias carteira_ativa carteira_inadimplida_arrastada ativo_problematico . 0 AC | Bancário | S1 | PJ | PJ - Administração pública, defesa e seguridad... | - | PJ - Micro | PJ - Capital de giro | Sem destinação específica | Flutuantes | ... | 1205677,52 | 2951544,24 | 7870784,64 | 7542835,28 | 14101828,32 | 0 | 0 | 33672670 | 0 | 0 | . 1 AC | Bancário | S1 | PJ | PJ - Administração pública, defesa e seguridad... | - | PJ - Micro | PJ - Financiamento de infraestrutura/desenvolv... | Com destinação específica | Flutuantes | ... | 1444428,27 | 4395932,84 | 11821330,14 | 11590794,08 | 19605221,57 | 0 | 0 | 48857706,9 | 0 | 0 | . 2 AC | Bancário | S1 | PJ | PJ - Administração pública, defesa e seguridad... | - | PJ - Micro | PJ - Financiamento de infraestrutura/desenvolv... | Com destinação específica | Pós-fixado | ... | 2341072,72 | 6807699,62 | 14364730,68 | 11159167,98 | 39279188,09 | 2717878,98 | 0 | 76669738,07 | 0 | 0 | . 3 AC | Bancário | S1 | PJ | PJ - Administração pública, defesa e seguridad... | - | PJ - Pequeno | PJ - Capital de giro rotativo | Sem destinação específica | Prefixado | ... | 0 | 498,66 | 0 | 0 | 0 | 0 | 0 | 498,66 | 0 | 0 | . 4 AC | Bancário | S1 | PJ | PJ - Administração pública, defesa e seguridad... | - | PJ - Pequeno | PJ - Outros créditos | Sem destinação específica | Prefixado | ... | 9147,12 | 18393,16 | 45713,32 | 39605,21 | 6497,9 | 0 | 2151,69 | 121508,4 | 0 | 0 | . 5 rows × 21 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.shape #Dimensão e proporção dos dados . (656958, 21) . Informa&#231;&#245;es do DataFrame . df.columns #Informações das colunas . Index([&#39;uf&#39;, &#39;tcb&#39;, &#39;sr&#39;, &#39;cliente&#39;, &#39;cnae_secao&#39;, &#39;cnae_subclasse&#39;, &#39;porte&#39;, &#39;modalidade&#39;, &#39;origem&#39;, &#39;indexador&#39;, &#39;numero_de_operacoes&#39;, &#39;a_vencer_ate_90_dias&#39;, &#39;a_vencer_de_91_ate_360_dias&#39;, &#39;a_vencer_de_361_ate_1080_dias&#39;, &#39;a_vencer_de_1081_ate_1800_dias&#39;, &#39;a_vencer_de_1801_ate_5400_dias&#39;, &#39;a_vencer_acima_de_5400_dias&#39;, &#39;vencido_acima_de_15_dias&#39;, &#39;carteira_ativa&#39;, &#39;carteira_inadimplida_arrastada&#39;, &#39;ativo_problematico&#39;], dtype=&#39;object&#39;) . df.info() #Informações do DataFrame . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 656958 entries, 0 to 656957 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 uf 656958 non-null object 1 tcb 656958 non-null object 2 sr 656958 non-null object 3 cliente 656958 non-null object 4 cnae_secao 656958 non-null object 5 cnae_subclasse 656958 non-null object 6 porte 656958 non-null object 7 modalidade 656958 non-null object 8 origem 656958 non-null object 9 indexador 656958 non-null object 10 numero_de_operacoes 656958 non-null object 11 a_vencer_ate_90_dias 656958 non-null object 12 a_vencer_de_91_ate_360_dias 656958 non-null object 13 a_vencer_de_361_ate_1080_dias 656958 non-null object 14 a_vencer_de_1081_ate_1800_dias 656958 non-null object 15 a_vencer_de_1801_ate_5400_dias 656958 non-null object 16 a_vencer_acima_de_5400_dias 656958 non-null object 17 vencido_acima_de_15_dias 656958 non-null object 18 carteira_ativa 656958 non-null object 19 carteira_inadimplida_arrastada 656958 non-null object 20 ativo_problematico 656958 non-null object dtypes: object(21) memory usage: 105.3+ MB . df.duplicated().sum() #verificação se há valores duplicados . 0 . df.isnull().sum() #verificação se há valores nulos . uf 0 tcb 0 sr 0 cliente 0 cnae_secao 0 cnae_subclasse 0 porte 0 modalidade 0 origem 0 indexador 0 numero_de_operacoes 0 a_vencer_ate_90_dias 0 a_vencer_de_91_ate_360_dias 0 a_vencer_de_361_ate_1080_dias 0 a_vencer_de_1081_ate_1800_dias 0 a_vencer_de_1801_ate_5400_dias 0 a_vencer_acima_de_5400_dias 0 vencido_acima_de_15_dias 0 carteira_ativa 0 carteira_inadimplida_arrastada 0 ativo_problematico 0 dtype: int64 . Limpeza dos dados . df.drop([&#39;modalidade&#39;,&#39;origem&#39;,&#39;indexador&#39;, &#39;a_vencer_ate_90_dias&#39;, &#39;a_vencer_de_91_ate_360_dias&#39;, &#39;a_vencer_de_361_ate_1080_dias&#39;, &#39;a_vencer_de_1081_ate_1800_dias&#39;, &#39;a_vencer_de_1801_ate_5400_dias&#39;, &#39;a_vencer_acima_de_5400_dias&#39;, &#39;vencido_acima_de_15_dias&#39;, &#39;cliente&#39;], axis=1, inplace=True) . df[&#39;carteira_ativa&#39;] = df[&#39;carteira_ativa&#39;].str.replace(&#39;,&#39;, &#39;.&#39;).astype(float) df[&#39;carteira_inadimplida_arrastada&#39;] = df[&#39;carteira_inadimplida_arrastada&#39;].str.replace(&#39;,&#39;, &#39;.&#39;).astype(float) df[&#39;ativo_problematico&#39;] = df[&#39;ativo_problematico&#39;].str.replace(&#39;,&#39;, &#39;.&#39;).astype(float) df[&#39;numero_de_operacoes&#39;] = df[&#39;numero_de_operacoes&#39;].str.replace(&#39;&lt;= 15&#39;, &#39;1&#39;).astype(int) df[&#39;possui_ap&#39;] = np.where(df[&#39;ativo_problematico&#39;] == 0, &#39;Não&#39;, &#39;Sim&#39;) df.dtypes . uf object tcb object sr object cnae_secao object cnae_subclasse object porte object numero_de_operacoes int64 carteira_ativa float64 carteira_inadimplida_arrastada float64 ativo_problematico float64 possui_ap object dtype: object . Analise Explorat&#243;ria . df2 = df.groupby([&#39;porte&#39;, &#39;possui_ap&#39;])[&#39;possui_ap&#39;].count() df3 = df2.groupby(level=0).apply(lambda x: 100 * x / float(x.sum())) print(df3) . porte possui_ap PJ - Grande Não 94.760298 Sim 5.239702 PJ - Micro Não 65.130758 Sim 34.869242 PJ - Médio Não 83.696810 Sim 16.303190 PJ - Pequeno Não 69.018913 Sim 30.981087 Name: possui_ap, dtype: float64 . distribution = pd.crosstab(df.porte, df.possui_ap, normalize=&#39;index&#39;) plt.figure( figsize=(10, 9)) # plotar o cumsum, com hue order reverso sns.barplot(data=distribution.cumsum(axis=1).stack().reset_index(name=&#39;Dist&#39;), x=&#39;porte&#39;, y=&#39;Dist&#39;, hue=&#39;possui_ap&#39;, hue_order = distribution.columns[::-1], dodge=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3f14f58b10&gt; . distribution = pd.crosstab(df.uf, df.possui_ap, normalize=&#39;index&#39;) plt.figure( figsize=(10, 9)) # plotar o cumsum, com hue order reverso sns.barplot(data=distribution.cumsum(axis=1).stack().reset_index(name=&#39;Dist&#39;), x=&#39;uf&#39;, y=&#39;Dist&#39;, hue=&#39;possui_ap&#39;, hue_order = distribution.columns[::-1], dodge=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3f11743c10&gt; . Transforma&#231;&#227;o de vari&#225;veis categ&#243;ricas em num&#233;ricas . let = LabelEncoder() df[&#39;uf&#39;] = let.fit_transform(df[&#39;uf&#39;]) df[&#39;tcb&#39;] = let.fit_transform(df[&#39;tcb&#39;]) df[&#39;sr&#39;] = let.fit_transform(df[&#39;sr&#39;]) df[&#39;cnae_secao&#39;] = let.fit_transform(df[&#39;cnae_secao&#39;]) df[&#39;cnae_subclasse&#39;] = let.fit_transform(df[&#39;cnae_subclasse&#39;]) df[&#39;porte&#39;] = let.fit_transform(df[&#39;porte&#39;]) df[&#39;possui_ap&#39;] = let.fit_transform(df[&#39;possui_ap&#39;]) df.head() . uf tcb sr cnae_secao cnae_subclasse porte numero_de_operacoes carteira_ativa carteira_inadimplida_arrastada ativo_problematico possui_ap . 0 0 | 0 | 0 | 0 | 0 | 1 | 1 | 33672670.00 | 0.0 | 0.0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 1 | 1 | 48857706.90 | 0.0 | 0.0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 1 | 1 | 76669738.07 | 0.0 | 0.0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 3 | 1 | 498.66 | 0.0 | 0.0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 3 | 1 | 121508.40 | 0.0 | 0.0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; p = df.hist(figsize = (20,20)) . df.shape . (656958, 11) . correlations = df.corr() . f, ax = plt.subplots(figsize = (20, 20)) sns.heatmap(correlations, annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3f14ca8910&gt; . Treinando e avaliando modelos . X = df.iloc[:, :-1].values y = df.iloc[:, -1].values . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0) . sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) . classifier = LogisticRegression() classifier.fit(X_train, y_train) . LogisticRegression() . y_pred = classifier.predict(X_test) y_result = np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1) . y_pred_prob = classifier.predict_proba(X_test) y_pred_prob = y_pred_prob[:,1] y_result_prob = np.concatenate((y_pred.reshape(len(y_pred),1), y_pred_prob.reshape(len(y_pred_prob),1)),1) . Matriz de confus&#227;o . cm = confusion_matrix(y_test, y_pred) sns.heatmap(cm, annot=True, fmt=&#39;g&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3f148fb090&gt; . print(accuracy_score(y_test, y_pred)) . 0.8558283609352167 . from sklearn.metrics import classification_report print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.83 1.00 0.91 94090 1 1.00 0.49 0.66 37302 accuracy 0.86 131392 macro avg 0.92 0.75 0.78 131392 weighted avg 0.88 0.86 0.84 131392 . prob_previsao = classifier.predict_proba(X_test)[:,-1] #tfp = taxa de falsos positivos #tvp = taxa verdadeiros positivos tfp, tvp, limite = roc_curve(y_test, y_pred) print(&#39;roc_auc&#39;, roc_auc_score(y_test, y_pred)) plt.subplots(1, figsize=(5,5)) plt.title(&#39;Curva ROC&#39;) plt.plot(tfp, tvp) plt.xlabel(&#39;Especificidade&#39;) plt.ylabel(&#39;Sensibilidade&#39;) plt.plot([0, 1], ls=&quot;--&quot;, c = &#39;red&#39;) plt.plot([0, 0], [1, 0], ls=&quot;--&quot;, c = &#39;green&#39;), plt.plot([1, 1], ls=&quot;--&quot;, c = &#39;green&#39;) plt.show() . roc_auc 0.7461588111647467 .",
            "url": "https://oemeferreira.github.io/meu-blog/awari/datacience/projeto/final/2022/06/10/Projeto-Final-Data-Science-Awari.html",
            "relUrl": "/awari/datacience/projeto/final/2022/06/10/Projeto-Final-Data-Science-Awari.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Coletando histórico de IPCA do Bacen via API",
            "content": "import requests import datetime import pandas as pd from matplotlib import pyplot as plt . url = &#39;http://api.bcb.gov.br/dados/serie/bcdata.sgs.16121/dados?formato=json&#39; . df = pd.read_json(url) print(df) . data valor 0 01/01/1991 20.27 1 01/02/1991 19.62 2 01/03/1991 13.45 3 01/04/1991 4.85 4 01/05/1991 8.15 .. ... ... 371 01/12/2021 0.91 372 01/01/2022 0.61 373 01/02/2022 1.10 374 01/03/2022 0.80 375 01/04/2022 0.59 [376 rows x 2 columns] . df.to_csv(&quot;dados_ipca.csv&quot;, index=False) . meses = df[&#39;data&#39;][-12:] valores = df[&#39;valor&#39;][-12:] plt.plot(meses, valores, &#39;r-o&#39;) plt.xticks(rotation=45) plt.show() .",
            "url": "https://oemeferreira.github.io/meu-blog/2022/05/11/api-bacen-ipca.html",
            "relUrl": "/2022/05/11/api-bacen-ipca.html",
            "date": " • May 11, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Coletar descrição de jobs no Linkedin com Selenium",
            "content": "1.Todas as importacoes . from selenium import webdriver from selenium.webdriver.common.by import By from time import sleep # Importa a função de tempo . 2.Todos os parametros . URL_LINKEDIN_DS = &#39;https://www.linkedin.com/jobs/ci%C3%AAncia-de-dados-vagas/?originalSubdomain=br&amp;position=1&amp;pageNum=0&#39; . 3. Execu&#231;&#227;o do c&#243;digo . if __name__ == &#39;__main__&#39;: driver = webdriver.Chrome() driver.implicitly_wait(10) driver.get(URL_LINKEDIN_DS) vagas = driver.find_elements_by_class_name(&#39;base-card&#39;) l_descricao = [] erros = 0 while True: for vaga in vagas: vaga.click() sleep(1) try: descricao = driver.find_element_by_class_name(&#39;description&#39;) l_descricao.append(descricao.text) except: print(&quot;Erro&quot;) erros +=1 pass vagas = driver.find_elements_by_class_name(&#39;base-card&#39;) if len(l_descricao)+erros == len(vagas): break #Fechar o google driver driver.quit() . descri_salvar = &#39; n&#39;.join(l_descricao) with open(&#39;descricoes_vagas.txt&#39;, &#39;w&#39;, encoding=&quot;utf-8&quot;) as f: f.write(descri_salvar) .",
            "url": "https://oemeferreira.github.io/meu-blog/2022/05/10/Coletando-Dados-do-Linkedin-com-Selenium.html",
            "relUrl": "/2022/05/10/Coletando-Dados-do-Linkedin-com-Selenium.html",
            "date": " • May 10, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Maria Esther Bueno",
            "content": "Escolha da Foto . Para a escolha da foto pensei que teria que ser alguma coisa de que eu gostasse, alguma coisa do Brasil e alguma coisa que fosse importante na história. . Então como gosto bastante de tenis, pesnsei em alguma foto de um tenista famoso, mas depois de muito procurar, lembrei da Maria Esther Bueno oi uma tenista brasileira, que atuou nas décadas de 1950, 1960 e 1970, sendo uma das raras tenistas a conquistar títulos em três décadas diferentes. . Maior nome do tênis brasileiro (incluindo homens e mulheres), eleita a melhor tenista do século XX da América Latina, e incluída em 2012 na posição 38 entre os 100 Melhores Tenistas da história (incluindo homens e mulheres) pelo canal Tennis Channel, em seus vinte anos de carreira, colecionou 589 títulos internacionais. . Portanto com essa mulher incrível do tenis brasileiro, resolvi realizar esse exercício de colorização utilizando o python na ferramenta colaboratory do Google. . Importando Deoldify Wrapper do Github . !wget https://raw.githubusercontent.com/awarischool/br-data-science/master/image-colorizer/deoldify_wrapper.py . --2022-05-05 02:03:52-- https://raw.githubusercontent.com/awarischool/br-data-science/master/image-colorizer/deoldify_wrapper.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2623 (2.6K) [text/plain] Saving to: ‘deoldify_wrapper.py’ deoldify_wrapper.py 100%[===================&gt;] 2.56K --.-KB/s in 0s 2022-05-05 02:03:52 (32.8 MB/s) - ‘deoldify_wrapper.py’ saved [2623/2623] . Importando biblioteca DeOldify . from deoldify_wrapper import DeOldify . Importing Libraries No module named &#39;deoldify&#39; DeOldify not found, installing.. Cloning DeOldify Repository... Opening DeOldify Folder Importing Libraries . /content/deoldify_wrapper.py:36: UserWarning: WARNING: GPU not available. Activate it on Colab at Edit &gt; Notebook Settings warnings.warn(&#39;WARNING: GPU not available. Activate it on Colab at Edit &gt; Notebook Settings&#39;) . Installing Colab requirements... Importing DeOldify Visualize module and FastAI Downloading Colorizer Model . Definindo a fun&#231;&#227;o . deo = DeOldify() . Initializing Colorizer . /usr/local/lib/python3.7/dist-packages/fastai/data_block.py:442: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning. warn(&#34;Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.&#34;) /usr/local/lib/python3.7/dist-packages/fastai/data_block.py:445: UserWarning: Your validation set is empty. If this is by design, use `split_none()` or pass `ignore_empty=True` when labelling to remove this warning. or pass `ignore_empty=True` when labelling to remove this warning.&#34;&#34;&#34;) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Downloading: &#34;https://download.pytorch.org/models/resnet101-63fe2227.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth . Done! . Colorizando a foto . deo.colorize(&#39;https://tenisbrasil.uol.com.br/fotos/2018/lendas/maria_esther_usopen_arquivo_int.jpe&#39;) .",
            "url": "https://oemeferreira.github.io/meu-blog/tenis/tenista/2022/05/04/Maria-Esther-Bueno-Colorizacao.html",
            "relUrl": "/tenis/tenista/2022/05/04/Maria-Esther-Bueno-Colorizacao.html",
            "date": " • May 4, 2022"
        }
        
    
  

  
  

  

  

  
  

  
  

  
      ,"page5": {
          "title": "Sobre Mim",
          "content": "Atualmente trabalho com atendimento ao público de pessoa jurídica, com reconhecimento em excelência no atendimento e vendas. . Grande interesse no campo de Data Science, onde tenho desenvolvido minhas competências a cada dia. . Habilidades Técnicas: ML usando Python (Pandas, NumPy, Matplotlib, Scikit-learn) SQL, Estatística, Mineração de Dados, Visualização de Dados, MS Excel. . Graduado em Marketing Digital com conhecimento em tráfego pago, SEO, redes sociais (Instagram, Youtube, Facebook), blog em wordpress. . Quando não estou trabalhando, estudando ou lendo, você pode me encontrar em um jogo de futebol, na academia, ouvindo podcasts e mexendo em projetos de ciência de dados. .",
          "url": "https://oemeferreira.github.io/meu-blog/sobre/",
          "relUrl": "/sobre/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://oemeferreira.github.io/meu-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}